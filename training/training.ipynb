{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from training_utils import CNN2D, spectra_stats, SpectraDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed has been set\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed seed value\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "pl.seed_everything(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "print(\"Seed has been set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN on Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "base_path = \"..\\preprocessed_dset\"\n",
    "data_path = os.path.join(base_path, \"spectrograms\")\n",
    "meta_path = os.path.join(base_path, \"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(..\\preprocessed_dset\\spectrograms) Mean: [0.68137387 0.66712049 0.64939852], Std: [0.13272135 0.1345285  0.13304515] (calculated and saved)\n"
     ]
    }
   ],
   "source": [
    "# Data stats for normalization\n",
    "    \n",
    "mean, std = spectra_stats(data_path, meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset -> Train: 3534, Test: 1165\n",
      "Dataset (Excluding colocated events) -> Train: 3434, Test: 1265\n",
      "\n",
      "Training batch shape:  torch.Size([32, 3, 33, 96])\n",
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset and DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dset = SpectraDataset(data_path=data_path,\n",
    "                            dataframe_path=meta_path,\n",
    "                            transform = transform,\n",
    "                            split=\"train\")\n",
    "test_dset = SpectraDataset(data_path=data_path,\n",
    "                            dataframe_path=meta_path,\n",
    "                            transform = transform,\n",
    "                            split=\"test\",\n",
    "                            get_metadata=True,\n",
    "                            same_amount=False)\n",
    "\n",
    "train_dset_c = SpectraDataset(data_path=data_path,\n",
    "                            dataframe_path=meta_path,\n",
    "                            transform = transform,\n",
    "                            split=\"train\",\n",
    "                            colocated=True)\n",
    "test_dset_c = SpectraDataset(data_path=data_path,\n",
    "                            dataframe_path=meta_path,\n",
    "                            transform = transform,\n",
    "                            split=\"test\",\n",
    "                            get_metadata=True,\n",
    "                            colocated=True,\n",
    "                            same_amount=False)\n",
    "\n",
    "# print dset dimensions\n",
    "print(f\"Dataset -> Train: {len(train_dset)}, Test: {len(test_dset)}\")\n",
    "print(f\"Dataset (Excluding colocated events) -> Train: {len(train_dset_c)}, Test: {len(test_dset_c)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define Dataloader\n",
    "\n",
    "train_data = DataLoader(train_dset, batch_size=32, shuffle=True)\n",
    "test_data = DataLoader(test_dset, batch_size=1, shuffle=False)\n",
    "sample_batch, _ = next(iter(train_data))  # Assuming labels exist, ignore them with \"_\"\n",
    "\n",
    "# Get the spatial shape (height, width) of the images\n",
    "sample_shape = sample_batch.shape[-2:]\n",
    "\n",
    "print(\"\\nTraining batch shape: \", sample_batch.shape) \n",
    "\n",
    "train_data_c = DataLoader(train_dset_c, batch_size=32, shuffle=True)\n",
    "test_data_c = DataLoader(test_dset_c, batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0: Train: 3030, Val: 504\n",
      "Split 1: Train: 3030, Val: 504\n",
      "Split 2: Train: 3030, Val: 504\n",
      "Split 3: Train: 3030, Val: 504\n",
      "Split 4: Train: 3030, Val: 504\n",
      "Split 5: Train: 3030, Val: 504\n",
      "Split 6: Train: 3024, Val: 510\n",
      "\n",
      "Split 0: Train: 2944, Val: 490\n",
      "Split 1: Train: 2944, Val: 490\n",
      "Split 2: Train: 2944, Val: 490\n",
      "Split 3: Train: 2944, Val: 490\n",
      "Split 4: Train: 2944, Val: 490\n",
      "Split 5: Train: 2944, Val: 490\n",
      "Split 6: Train: 2940, Val: 494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the splits for the cross-validation\n",
    "\n",
    "def split_train_cross_val(dataset, n_splits=7):\n",
    "    splits = []\n",
    "    size = (len(dataset) // n_splits)\n",
    "    last_size = len(dataset) - size*(n_splits-1)\n",
    "    out = random_split(dataset, [size]*(n_splits - 1) + [last_size])\n",
    "    for idx in range(n_splits):\n",
    "        train = torch.utils.data.ConcatDataset([out[i] for i in range(n_splits) if i != idx])\n",
    "        val = out[idx]\n",
    "        splits.append((train, val))\n",
    "        print(f\"Split {idx}: Train: {len(train)}, Val: {len(val)}\")\n",
    "    print()\n",
    "    return splits\n",
    "\n",
    "\n",
    "splits = split_train_cross_val(train_dset)\n",
    "splits_c = split_train_cross_val(train_dset_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type             | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0  | conv1      | Conv2d           | 896    | train\n",
      "1  | BatchNorm1 | BatchNorm2d      | 64     | train\n",
      "2  | conv2      | Conv2d           | 18.5 K | train\n",
      "3  | BatchNorm2 | BatchNorm2d      | 128    | train\n",
      "4  | conv3      | Conv2d           | 73.9 K | train\n",
      "5  | BatchNorm3 | BatchNorm2d      | 256    | train\n",
      "6  | maxpool    | MaxPool2d        | 0      | train\n",
      "7  | relu       | ReLU             | 0      | train\n",
      "8  | dropout    | Dropout          | 0      | train\n",
      "9  | flatten    | Flatten          | 0      | train\n",
      "10 | fc1        | Linear           | 3.1 M  | train\n",
      "11 | fc2        | Linear           | 258    | train\n",
      "12 | criterion  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.959    Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for fold 0...\n",
      "Epoch 4: 100%|██████████| 95/95 [00:13<00:00,  6.82it/s, v_num=2, train_loss=0.0511, val_loss=0.0181, val_acc=0.996]   \n",
      "New best model found for fold 0 with valid accuracy: 0.9960\n",
      "Training model for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 95/95 [00:11<00:00,  8.19it/s, v_num=3, train_loss=2.55e-5, val_loss=0.0229, val_acc=0.996] \n",
      "Training model for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 95/95 [00:13<00:00,  6.80it/s, v_num=4, train_loss=3.01e-5, val_loss=0.00598, val_acc=0.998] \n",
      "New best model found for fold 2 with valid accuracy: 0.9980\n",
      "Training model for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 95/95 [00:14<00:00,  6.70it/s, v_num=5, train_loss=1.59e-5, val_loss=0.0106, val_acc=0.994]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for fold 4...\n",
      "Epoch 5: 100%|██████████| 95/95 [00:14<00:00,  6.41it/s, v_num=6, train_loss=0.000234, val_loss=0.00736, val_acc=0.996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for fold 5...\n",
      "Epoch 7: 100%|██████████| 95/95 [00:16<00:00,  5.92it/s, v_num=7, train_loss=9.44e-6, val_loss=0.00623, val_acc=0.998] \n",
      "Training model for fold 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 95/95 [00:15<00:00,  6.26it/s, v_num=8, train_loss=0.000202, val_loss=0.0044, val_acc=0.996] \n"
     ]
    }
   ],
   "source": [
    "# Training with cross-validation (using EarlyStopping and ModelCheckpoint)\n",
    "\n",
    "best_model_accuracy = 0\n",
    "best_model_path = None\n",
    "\n",
    "for idx, (train_dset, val_dset) in enumerate(splits):\n",
    "    model = CNN2D(input_dim=sample_shape)\n",
    "    \n",
    "    print(f\"Training model for fold {idx}...\")\n",
    "    early_stop_callback = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, dirpath='model_checkpoints', filename=f'best_model_fold_{idx}')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10, \n",
    "        accelerator='gpu',\n",
    "        devices=-1, \n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        enable_model_summary = (False if idx > 0 else True)\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, DataLoader(train_dset, batch_size=32, shuffle=True), DataLoader(val_dset, batch_size=32, shuffle=False))\n",
    "    \n",
    "    val_acc = trainer.callback_metrics.get('val_acc', 0).item()  # Extract validation accuracy\n",
    "    \n",
    "    if val_acc > best_model_accuracy:\n",
    "        best_split = idx\n",
    "        best_model_accuracy = val_acc\n",
    "        best_model_path = checkpoint_callback.best_model_path\n",
    "        print(f\"New best model found for fold {idx} with valid accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = r\"..\\training\\model_checkpoints\\best_model_fold_2.ckpt\" # Use the path for the best model found in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_metrics(model_path, test_dset, input_dim):\n",
    "    model = CNN2D.load_from_checkpoint(model_path, input_dim=input_dim, n_classes=2)\n",
    "    trainer = pl.Trainer(enable_model_summary=False, logger=False, enable_progress_bar=False)\n",
    "    trainer.test(model, test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics for Spectrogram dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\miche\\OneDrive\\Desktop\\EQML Project\\CNN-Explainability-on-Earthquakes\\CNN_EQML\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 0.9966\n",
      "Total Precision: 0.9953\n",
      "Total Recall: 0.9953\n",
      "Total F1 Score: 0.9953\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Metrics for Spectrogram dataset:\")\n",
    "compute_test_metrics(best_model_path, test_data, sample_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Test Dataset for Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 300 Random Traces Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick = 300 # Number of correct predicted random samples to pick for each class\n",
    "correct_aftershock_samples = []\n",
    "correct_foreshock_samples = []\n",
    "\n",
    "model = CNN2D.load_from_checkpoint(best_model_path, input_dim=sample_shape, n_classes=2)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for img, label, trace_name, *_ in tqdm(test_data, desc=\"Checking correct samples\"):\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    output = model(img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == label)\n",
    "    for i in range(len(correct)):\n",
    "        if correct[i]:\n",
    "            if label[i] == 1:\n",
    "                correct_aftershock_samples.append(trace_name[i])\n",
    "            else:\n",
    "                correct_foreshock_samples.append(trace_name[i])\n",
    "\n",
    "    if len(correct_aftershock_samples) >= pick and len(correct_foreshock_samples) >= pick:\n",
    "        break\n",
    "\n",
    "shuffle(correct_aftershock_samples)\n",
    "shuffle(correct_foreshock_samples)\n",
    "correct_aftershock_samples = correct_aftershock_samples[:pick]\n",
    "correct_foreshock_samples = correct_foreshock_samples[:pick]\n",
    "\n",
    "print(f\"Correct aftershock samples: {len(correct_aftershock_samples)}\")\n",
    "print(f\"Correct foreshock samples: {len(correct_foreshock_samples)}\")\n",
    "\n",
    "\n",
    "output_explain_path = r\"..\\explainability\\samples\\aftershocks\"\n",
    "os.makedirs(output_explain_path, exist_ok=True)\n",
    "for name in tqdm(correct_aftershock_samples, desc=\"Copying aftershock samples\"):\n",
    "    img_path = os.path.join(data_path, name + \".png\")\n",
    "    \n",
    "    # Copy and Paste the image to the output folder\n",
    "    os.system(f\"copy {img_path} {output_explain_path}\")\n",
    "\n",
    "output_explain_path = r\"..\\explainability\\samples\\foreshocks\"\n",
    "os.makedirs(output_explain_path, exist_ok=True)\n",
    "for name in tqdm(correct_foreshock_samples, desc=\"Copying foreshock samples\"):\n",
    "    img_path = os.path.join(data_path, name + \".png\")\n",
    "    \n",
    "    # Copy and Paste the image to the output folder\n",
    "    os.system(f\"copy {img_path} {output_explain_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misclassified Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking misclassified samples: 100%|██████████| 1165/1165 [00:11<00:00, 101.14it/s]\n",
      "Copying misclassified samples: 100%|██████████| 4/4 [00:00<00:00, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Foreshock_Value  Aftershock_Value\n",
      "Trace Name                                             \n",
      "NRCA.IV.100529025_EV         1.601379         -0.165925\n",
      "NRCA.IV.100460587_EV         0.617522          0.254362\n",
      "NRCA.IV.100021270_EV        -0.802994          1.804882\n",
      "NRCA.IV.100024219_EV         0.020267          0.684034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN2D.load_from_checkpoint(best_model_path, input_dim=sample_shape, n_classes=2)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "incorrect_samples = []\n",
    "model_output_dict = {}\n",
    "\n",
    "\n",
    "for img, label, trace_name, *_ in tqdm(test_data, desc=\"Checking misclassified samples\"):\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    output = model(img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    misclassified = (predicted != label).squeeze()\n",
    "\n",
    "    # Ensure misclassified is iterable\n",
    "    if misclassified.dim() == 0:  # Handles batch size = 1 case\n",
    "        misclassified = misclassified.unsqueeze(0)\n",
    "\n",
    "    if misclassified.sum() == 0:\n",
    "        for i, is_misclassified in enumerate(misclassified):\n",
    "            if is_misclassified.item():  # Convert to Python boolean\n",
    "                incorrect_samples.append(trace_name[i])\n",
    "                model_output_dict[trace_name[i]] = output[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        print(\"No misclassified samples found in this batch.\")\n",
    "\n",
    "\n",
    "output_misclassified_path = r\"..\\explainability\\samples\\misclassified\"\n",
    "os.makedirs(output_misclassified_path, exist_ok=True)\n",
    "\n",
    "if not incorrect_samples:\n",
    "    print(\"No misclassified samples found.\")\n",
    "else:\n",
    "    for trace_name in tqdm(incorrect_samples, desc=\"Copying misclassified samples\"):\n",
    "        img_path = os.path.join(data_path, trace_name + \".png\")\n",
    "        \n",
    "        # Copy and Paste the image to the output folder\n",
    "        os.system(f\"copy {img_path} {output_misclassified_path}\")\n",
    "\n",
    "    # Save dictionary to a CSV file\n",
    "    df = pd.DataFrame(model_output_dict).T\n",
    "    df.columns = [\"Foreshock_Value\", \"Aftershock_Value\"]\n",
    "    df.index.name = \"Trace Name\"\n",
    "    print(df)\n",
    "    df.to_csv(r\"..\\explainability\\samples\\misclassified\\model_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_EQML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
